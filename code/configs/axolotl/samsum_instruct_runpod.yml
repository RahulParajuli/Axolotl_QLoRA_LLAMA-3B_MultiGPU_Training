# Axolotl configuration for fine‑tuning an instruction‑tuned LLaMA‑3B model on the SamSum dataset.
# This config is compatible with the official Axolotl Docker image on RunPod.
# It references the helper utilities added in `utils/samsum_helpers.py` but does not require any code changes.

base_model: meta-llama/Meta-Llama-3B-Instruct

# Quantization (QLoRA) settings
load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

# LoRA configuration
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules:
  - q_proj
  - v_proj

# Training hyper‑parameters
learning_rate: 2e-4
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
optim: adamw_bnb_8bit
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.01

# Mixed‑precision / device settings
bf16: true
gradient_checkpointing: true

# Dataset configuration – SamSum (dialogue summarization)
dataset:
  name: samsum
  splits:
    train: all
    validation: all
    test: all
  # The SamSum dataset already provides `dialogue` and `summary` columns.
  # The helper `load_samsum_dataset` will format them as an instruction‑response pair.

# Tokenizer settings – use the same tokenizer as the base model
tokenizer:
  use_fast: true
  padding_side: right
  pad_token: "<pad>"

# Output / logging
output_dir: ./outputs/samsum_instruct
logging_steps: 10
eval_steps: 100
save_steps: 200
save_total_limit: 2
report_to: ["wandb", "tensorboard"]
wandb_project: samsum_instruct
wandb_entity: your_wandb_entity

# RunPod specific settings (environment variables can be set in the pod)
# No additional changes required; the Docker image will read this YAML.
